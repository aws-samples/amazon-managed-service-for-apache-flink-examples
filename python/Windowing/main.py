# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: MIT-0
# -*- coding: utf-8 -*-

"""
main.py
~~~~~~~~~~~~~~~~~~~
This module:
    1. Creates the execution environment
    2. Set any special configuration for local mode (e.g. when running in the IDE)
    3. Retrieve runtime configuration
    4. Define a source table to generate data using DataGen connector
    5. Define sink table and windowing aggregation query, one per window type: tumbling or sliding windows,
        processing-time or event-time.
"""

from pyflink.table import EnvironmentSettings, TableEnvironment
import os
import json

#######################################
# 1. Creates the execution environment
#######################################

env_settings = EnvironmentSettings.in_streaming_mode()
table_env = TableEnvironment.create(env_settings)

# This location is only available when running on Managed Flink.
# NOTE: the content of this file is based on the application configuration, and NOT on the
# application_properties.json file included in the project. The local file is only used for local development.
APPLICATION_PROPERTIES_FILE_PATH = "/etc/flink/application_properties.json"

# Set the environment variable IS_LOCAL=true in your local development environment,
# or in the run profile of your IDE: the application relies on this variable to run in local mode (as a standalone
# Python application, as opposed to running in a Flink cluster).
# Differently from Java Flink, PyFlink cannot automatically detect when running in local mode
is_local = (
    True if os.environ.get("IS_LOCAL") else False
)

##############################################
# 2. Set special configuration for local mode
##############################################

if is_local:
    # Load the configuration from the json file included in the project
    APPLICATION_PROPERTIES_FILE_PATH = "application_properties.json"

    # Point to the fat-jar generated by Maven, containing all jar dependencies (e.g. connectors)
    CURRENT_DIR = os.path.dirname(os.path.realpath(__file__))
    table_env.get_config().get_configuration().set_string(
        "pipeline.jars",
        # For local development (only): use the fat-jar containing all dependencies, generated by `mvn package`
        "file:///" + CURRENT_DIR + "/target/pyflink-dependencies.jar",
    )

def get_application_properties():
    if os.path.isfile(APPLICATION_PROPERTIES_FILE_PATH):
        with open(APPLICATION_PROPERTIES_FILE_PATH, "r") as file:
            contents = file.read()
            properties = json.loads(contents)
            return properties
    else:
        print('A file at "{}" was not found'.format(APPLICATION_PROPERTIES_FILE_PATH))

def property_map(props, property_group_id):
    for prop in props:
        if prop["PropertyGroupId"] == property_group_id:
            return prop["PropertyMap"]


def main():

    #####################################
    # 3. Retrieve runtime configuration
    #####################################


    props = get_application_properties()
    tumbling_windows_proc_time_output_stream_name = property_map(props, "OutputStream0")["stream.name"]
    tumbling_windows_proc_time_output_stream_region = property_map(props, "OutputStream0")["aws.region"]
    tumbling_windows_event_time_output_stream_name = property_map(props, "OutputStream1")["stream.name"]
    tumbling_windows_event_time_output_stream_region = property_map(props, "OutputStream1")["aws.region"]
    sliding_windows_proc_time_output_stream_name = property_map(props, "OutputStream2")["stream.name"]
    sliding_windows_proc_time_output_stream_region = property_map(props, "OutputStream2")["aws.region"]
    sliding_windows_event_time_output_stream_name = property_map(props, "OutputStream3")["stream.name"]
    sliding_windows_event_time_output_stream_region = property_map(props, "OutputStream3")["aws.region"]

    #################################################
    # 4. Define input table using datagen connector
    #################################################

    # In a real application, this table will probably be connected to a source stream, using for example the 'kinesis'
    # connector.
    table_env.execute_sql("""CREATE TABLE sensor_readings (
                sensor_id INT,
                temperature NUMERIC(6,2),
                measurement_time TIMESTAMP(3),
                proc_time AS PROCTIME(),
                WATERMARK FOR measurement_time AS measurement_time - INTERVAL '5' SECOND
              )
              PARTITIONED BY (sensor_id)
              WITH (
                'connector' = 'datagen',
                'fields.sensor_id.min' = '10',
                'fields.sensor_id.max' = '20',
                'fields.temperature.min' = '0',
                'fields.temperature.max' = '100'
              ) """)

    # 5. Define sink tables and window aggregation queries, one per window type
    # Note that this code is for demonstration purposes only, to compare different types of windows.
    # In a real application you will probably not repeat multiple times practically identical code


    ##########################################
    # 5.1 Tumbling windows in processing time
    ##########################################

    # Sink table
    table_env.execute_sql("""
            CREATE TABLE tumbling_proc_time_out (
                sensor_id INT,
                avg_temp NUMERIC(6,2),
                window_end TIMESTAMP_LTZ(3) NOT NULL
              )
              PARTITIONED BY (sensor_id)
              WITH (
                'connector' = 'kinesis',
                'stream' = '{0}',
                'aws.region' = '{1}',
                'format' = 'json',
                'json.timestamp-format.standard' = 'ISO-8601'
              ) """.format(tumbling_windows_proc_time_output_stream_name, tumbling_windows_proc_time_output_stream_region))

    # Processing-time tumbling window aggregation
    table_result0 = table_env.execute_sql("""
            INSERT INTO tumbling_proc_time_out
            SELECT 
                sensor_id, 
                AVG(temperature) AS avg_temp, 
                TUMBLE_END(proc_time, INTERVAL '10' SECONDS) AS window_end
            FROM sensor_readings
            GROUP BY TUMBLE(proc_time, INTERVAL '10' SECONDS), sensor_id
    """)


    ##########################################
    # 5.2 Tumbling windows in event time
    ##########################################

    # Sink table
    table_env.execute_sql("""
            CREATE TABLE tumbling_event_time_out (
                sensor_id INT,
                avg_temp NUMERIC(6,2),
                window_end TIMESTAMP_LTZ(3) NOT NULL
              )
              PARTITIONED BY (sensor_id)
              WITH (
                'connector' = 'kinesis',
                'stream' = '{0}',
                'aws.region' = '{1}',
                'format' = 'json',
                'json.timestamp-format.standard' = 'ISO-8601'
              ) """.format(tumbling_windows_event_time_output_stream_name, tumbling_windows_event_time_output_stream_region))

    # Event-time tumbling window aggregation
    table_result1 = table_env.execute_sql("""
            INSERT INTO tumbling_event_time_out
            SELECT 
                sensor_id, 
                AVG(temperature) AS avg_temp, 
                TUMBLE_END(measurement_time, INTERVAL '10' SECONDS) AS window_end
            FROM sensor_readings
            GROUP BY TUMBLE(measurement_time, INTERVAL '10' SECONDS), sensor_id
    """)

    ##########################################
    # 5.3 Sliding windows in processing time
    ##########################################

    # Sink table
    table_env.execute_sql("""
            CREATE TABLE sliding_proc_time_out (
                sensor_id INT,
                avg_temp NUMERIC(6,2),
                window_end TIMESTAMP_LTZ(3) NOT NULL
              )
              PARTITIONED BY (sensor_id)
              WITH (
                'connector' = 'kinesis',
                'stream' = '{0}',
                'aws.region' = '{1}',
                'format' = 'json',
                'json.timestamp-format.standard' = 'ISO-8601'
              ) """.format(sliding_windows_proc_time_output_stream_name, sliding_windows_proc_time_output_stream_region))

    # Processing-time sliding (or "hopping") window aggregation
    # Windows have fixed durations of 10 seconds, and slide (or "hop") of 2 seconds
    table_result2 = table_env.execute_sql("""
            INSERT INTO sliding_proc_time_out
            SELECT 
                sensor_id, 
                AVG(temperature) AS avg_temp, 
                HOP_END(proc_time, INTERVAL '2' SECONDS, INTERVAL '10' SECONDS) AS window_end
            FROM sensor_readings
            GROUP BY HOP(proc_time, INTERVAL '2' SECONDS, INTERVAL '10' SECONDS), sensor_id
    """)


    ##########################################
    # 5.4 Sliding windows in event time
    ##########################################

    # Sink table
    table_env.execute_sql("""
            CREATE TABLE sliding_event_time_out (
                sensor_id INT,
                avg_temp NUMERIC(6,2),
                window_end TIMESTAMP_LTZ(3) NOT NULL
              )
              PARTITIONED BY (sensor_id)
              WITH (
                'connector' = 'kinesis',
                'stream' = '{0}',
                'aws.region' = '{1}',
                'format' = 'json',
                'json.timestamp-format.standard' = 'ISO-8601'
              ) """.format(sliding_windows_event_time_output_stream_name, sliding_windows_event_time_output_stream_region))

    # Event-time sliding (or "hopping") window aggregation
    # Windows have fixed durations of 10 seconds, and slide (or "hop") of 2 seconds
    table_result3 = table_env.execute_sql("""
            INSERT INTO sliding_event_time_out
            SELECT 
                sensor_id, 
                AVG(temperature) AS avg_temp, 
                HOP_END(measurement_time, INTERVAL '2' SECONDS, INTERVAL '10' SECONDS) AS window_end
            FROM sensor_readings
            GROUP BY HOP(measurement_time, INTERVAL '2' SECONDS, INTERVAL '10' SECONDS), sensor_id
    """)

    # When running locally, as a standalone Python application, you must instruct Python not to exit at the end of the
    # main() method, otherwise the job will stop immediately.
    # When running the job deployed in a Flink cluster or in Amazon Managed Service for Apache Flink, the main() method
    # must end once the flow has been defined and handed over to the Flink framework to run.
    if is_local:
        table_result1.wait()

if __name__ == "__main__":
    main()
