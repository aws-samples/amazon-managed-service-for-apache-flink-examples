## Examples of windowing aggregations, with PyFlink/SQL

Example showing a basic PyFlink job doing data aggregation over time windows.

* Flink version: 1.18
* Flink API: Table API & SQL
* Flink Connectors: Kinesis Connector
* Language: Python (3.10)

The application demonstrate the behaviour some of the windowing primitives supported by Apache Flink. 

The job can run both on Amazon Managed Service for Apache Flink, and locally for development.

### Other requirements

#### Dev and build environment requirements

* Pyhon 3.10
* Install `apache-flink==1.18.1`
* Java JDK 11+ and Maven: Maven is used to download and package any required Flink dependencies, e.g. connectors, and
  to package the application as `.zip` file, for deployment to Amazon Managed Service for Apache Flink.

#### Runtime dependencies

The application expects 4 Kinesis Data Streams. The default names are:
* `SlidingWindowProcessingTimeOutput`
* `SlidingWindowEventTimeOutput`
* `TumblingWindowProcessingTimeOutput`
* `TumblingWindowEventTimeOutput`

#### IAM permissions

The application must have sufficient permissions to publish data to the Kinesis Data Streams.

When running locally, you need active valid AWS credentials that allow publishing data to the Streams.


### Application structure

Synthetic data are generated by the application itself using the [DataGen](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/table/datagen/) connector.
No external data generator is required.

The application demonstrates data aggregation 4 types of windows:

* Sliding Window based on processing time
* Sliding Window based on event time
* Tumbling Window based on processing time
* Tumbling Window based on event time

The result of each aggregation is sent to a different Kinesis Data Streams, as JSON, and can be easily inspected using the 
Data Viewer from the Kinesis Data Stream console.

### Runtime configuration

* Local development: reads [application_properties.json](./application_properties.json)
* Deployed on Amazon Managed Service for Apache Fink: set up Runtime Properties, using Group ID and property names based on the content of [application_properties.json](./application_properties.json)


TODO Explain dependencies
TODO Explain packaging
TODO Explain lifecycles: 1/ local dev, 2/ MSF deployment
